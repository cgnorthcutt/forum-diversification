{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import unicode_literals\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Google's pre-trained Word2Vec model.\n",
    "model = gensim.models.Word2Vec.load_word2vec_format('/Users/cgn/Downloads//GoogleNews-vectors-negative300.bin', binary=True)  \n",
    "\n",
    "vocab = np.array(model.vocab.keys())\n",
    "word_vector=[]\n",
    "for w in vocab:\n",
    "    #vector=model[w] / np.sum(model[w])\n",
    "    word_vector.append(model[w])\n",
    "\n",
    "del model\n",
    "w2v_matrix=np.asarray(word_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get list of comments\n",
    "df = pd.read_csv('../data/HarvardX__HDS_3221_2X__1T2016_scored_forum_responses_and_features.csv.gz', compression='gzip')\n",
    "comments = list(df.body.apply(lambda x: unicode(x, 'utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed reduction. Dumping to pickle.\n"
     ]
    }
   ],
   "source": [
    "# Creation of reduced w2v_matrix model with 1-gram through 6-gram\n",
    "\n",
    "all_txt = \" \".join(comments)\n",
    "translate_table = dict((ord(char), None) for char in string.punctuation)   \n",
    "all_words = all_txt.translate(translate_table).split()\n",
    "\n",
    "ngram_start = 1\n",
    "ngram_end = 6\n",
    "all_words_ngram = [\"_\".join(all_words[i:i + ngram]) for ngram in range(ngram_start, ngram_end +1) for i in range(len(all_words) - (ngram - 1))]\n",
    "unique_words_list = list(set(all_words_ngram))\n",
    "\n",
    "# Build hash map from word to index\n",
    "vocab_map = dict(zip(vocab, range(len(vocab))))\n",
    "\n",
    "# Get indices of items in vocab that are contained in unique_words_list\n",
    "idx_with_Nones = np.array([vocab_map.get(word) for word in unique_words_list])\n",
    "idx = [i for i in idx_with_Nones if i is not None]\n",
    "\n",
    "# Reduce w2v_matrix size by limiting to all unique words in this course.\n",
    "vocab = vocab[idx]\n",
    "w2v_matrix = w2v_matrix[idx]\n",
    "\n",
    "print(\"Completed reduction. Dumping to pickle.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump( w2v_matrix, open( \"../data/w2v_matrix_for_HarvardX__HDS_3221_2X__1T2016.p\", \"wb\" ) )\n",
    "pickle.dump( vocab, open( \"../data/vocab_for_HarvardX__HDS_3221_2X__1T2016.p\", \"wb\" ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
