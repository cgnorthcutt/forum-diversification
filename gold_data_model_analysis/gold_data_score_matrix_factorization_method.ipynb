{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA, TruncatedSVD,NMF\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.preprocessing import Normalizer\n",
    "import os  # for os.path.basename\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import unicode_literals\n",
    "from __future__ import print_function\n",
    "\n",
    "from scipy.stats import rankdata\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sys\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from make_pairwise_gold_metric_scores import compute_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Word2Vec Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_matrix = pickle.load( open( \"w2v_matrix_for_HarvardX__HDS_3221_2X__1T2016.p\", \"rb\" ) )\n",
    "vocab = np.array(pickle.load( open( \"vocab_for_HarvardX__HDS_3221_2X__1T2016.p\", \"rb\" ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Train and  Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gold_matrix_train = pd.read_csv('gold_matrix_train_HarvardX__HDS_3221_2X__1T2016.csv.gz', compression='gzip').as_matrix()\n",
    "df_gold_train= pd.read_csv('gold_data_train_HarvardX__HDS_3221_2X__1T2016.csv.gz', compression='gzip')\n",
    "gold_matrix_test= pd.read_csv(\"gold_matrix_test_HarvardX__HDS_3221_2X__1T2016.csv.gz\", compression='gzip').as_matrix()\n",
    "df_gold_test= pd.read_csv('gold_data_test_HarvardX__HDS_3221_2X__1T2016.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get contntent Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_content=df_gold_test.body.values\n",
    "train_content=df_gold_train.body.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import os\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "token_dict = {}\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stop_words=[]\n",
    "for i in xrange(len(stopwords)):\n",
    "    w=stopwords[i].encode('ascii','ignore')\n",
    "    stop_words.append(w)\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for w in tokens:\n",
    "        w_stem=lancaster_stemmer.stem(w)\n",
    "        w_stem=wordnet_lemmatizer.lemmatize(w_stem)\n",
    "        if not  w_stem.isdigit():\n",
    "            stemmed.append(w_stem)\n",
    "   \n",
    "    return stemmed\n",
    "def tokenize(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [word for word in tokens if len(word) > 2]\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [word for word in tokens if word in vocab]\n",
    "    tokens = [x for x in tokens if not any(c.isdigit() for c in x)]\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nBOW+Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(vocabulary=vocab, tokenizer=tokenize, stop_words=stop_words)\n",
    "train_df = vectorizer.fit_transform(train_content)\n",
    "test_df = vectorizer.fit_transform(test_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_sim(df):\n",
    "    A=df.dot(w2v_matrix)\n",
    "    A=normalize(A,norm=\"l2\")\n",
    "    simdf=pd.DataFrame(np.dot(A,A.transpose()))\n",
    "    return MinMaxScaler().fit_transform(simdf)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pairwise_cosine_similarity_train=cosine_sim(train_df)\n",
    "pairwise_cosine_similarity_test=cosine_sim(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sim_testdf=pd.DataFrame(pairwise_cosine_similarity_test)\n",
    "sim_testdf.to_csv(\"nBOW_sim_test.csv\")\n",
    "sim_traindf=pd.DataFrame(pairwise_cosine_similarity_train)\n",
    "sim_traindf.to_csv(\"nBOW_sim_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare Training Data and Validation Data in Similarity Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Quantile (Rank) Difference score: 0.638652329885 - 0.47208221253 = 0.166570117355\n",
      "Pairwise Binary Logistic Regression Accuracy score: 0.815278216339\n",
      "\n",
      "The next test uses parameter optimization over a random forest\n",
      "classifier's parameters and may take 30s to 2 min to run.\n",
      "\n",
      "Pairwise Binary Random Forest Accuracy score: 0.815506136025\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>logreg_acc_pairwise_binary</th>\n",
       "      <td>0.815278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median_quantile_diff</th>\n",
       "      <td>0.166570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random_forest_acc_pairwise_binary</th>\n",
       "      <td>0.815506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Score\n",
       "logreg_acc_pairwise_binary         0.815278\n",
       "median_quantile_diff               0.166570\n",
       "random_forest_acc_pairwise_binary  0.815506"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "metrics = compute_metrics(pairwise_cosine_similarity_train, pairwise_cosine_similarity_test, gold_matrix_train, df_gold_train, gold_matrix_test, df_gold_test)\n",
    "pretty_metrics = pd.DataFrame(pd.Series(metrics), columns = [\"Score\"])\n",
    "pretty_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tfidf+ Matric Factorization Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform to Vector Space Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words=stop_words)\n",
    "testdf_tfs = tfidf.fit_transform(test_content)\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words=stop_words)\n",
    "traindf_tfs = tfidf.fit_transform(train_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Quantile (Rank) Difference score: 0.788675715395 - 0.449050453347 = 0.339625262048\n",
      "Pairwise Binary Logistic Regression Accuracy score: 0.775021901657\n",
      "\n",
      "The next test uses parameter optimization over a random forest\n",
      "classifier's parameters and may take 30s to 2 min to run.\n",
      "\n",
      "Pairwise Binary Random Forest Accuracy score: 0.766638730677\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>logreg_acc_pairwise_binary</th>\n",
       "      <td>0.775022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median_quantile_diff</th>\n",
       "      <td>0.339625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random_forest_acc_pairwise_binary</th>\n",
       "      <td>0.766639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Score\n",
       "logreg_acc_pairwise_binary         0.775022\n",
       "median_quantile_diff               0.339625\n",
       "random_forest_acc_pairwise_binary  0.766639"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simtfs=pd.DataFrame(cosine_similarity( testdf_tfs,testdf_tfs))\n",
    "pairwise_cosine_similarity_test= Normalizer(copy=False).fit_transform(simtfs)\n",
    "tfidf_sim=pd.DataFrame(pairwise_cosine_similarity_test)\n",
    "tfidf_sim.to_csv(\"tfidf_sim_test.csv\")\n",
    "\n",
    "simtfs=pd.DataFrame(cosine_similarity( traindf_tfs, traindf_tfs))\n",
    "pairwise_cosine_similarity_train= Normalizer(copy=False).fit_transform(simtfs)\n",
    "tfidf_sim=pd.DataFrame(pairwise_cosine_similarity_train)\n",
    "tfidf_sim.to_csv(\"tfidf_sim_train.csv\")\n",
    "\n",
    "metrics = compute_metrics(pairwise_cosine_similarity_train, pairwise_cosine_similarity_test, gold_matrix_train, df_gold_train, gold_matrix_test, df_gold_test)\n",
    "pretty_metrics = pd.DataFrame(pd.Series(metrics), columns = [\"Score\"])\n",
    "pretty_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF+LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Quantile (Rank) Difference score: 0.863058254847 - 0.430085636074 = 0.432972618773\n",
      "Pairwise Binary Logistic Regression Accuracy score: 0.867082461818\n",
      "\n",
      "The next test uses parameter optimization over a random forest\n",
      "classifier's parameters and may take 30s to 2 min to run.\n",
      "\n",
      "Pairwise Binary Random Forest Accuracy score: 0.867239156602\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>logreg_acc_pairwise_binary</th>\n",
       "      <td>0.867082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median_quantile_diff</th>\n",
       "      <td>0.432973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random_forest_acc_pairwise_binary</th>\n",
       "      <td>0.867239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Score\n",
       "logreg_acc_pairwise_binary         0.867082\n",
       "median_quantile_diff               0.432973\n",
       "random_forest_acc_pairwise_binary  0.867239"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def TFIDF_LSA(eigenvector,df):\n",
    "    lsa = TruncatedSVD(eigenvector, algorithm = 'arpack')\n",
    "    A = lsa.fit_transform(df)\n",
    "    A=normalize(A,norm=\"l2\")\n",
    "    simdf=pd.DataFrame(np.dot(A,A.transpose()))\n",
    "    return MinMaxScaler().fit_transform(simdf)\n",
    "\n",
    "eigenvector=20\n",
    "pairwise_cosine_similarity_test=TFIDF_LSA(eigenvector,testdf_tfs)\n",
    "lsa_sim=pd.DataFrame(pairwise_cosine_similarity_test)\n",
    "lsa_sim.to_csv(\"lsa_sim_test.csv\")\n",
    "pairwise_cosine_similarity_train=TFIDF_LSA(eigenvector,traindf_tfs)\n",
    "lsa_sim=pd.DataFrame(pairwise_cosine_similarity_train)\n",
    "lsa_sim.to_csv(\"lsa_sim_train.csv\")\n",
    "\n",
    "metrics = compute_metrics(pairwise_cosine_similarity_train, pairwise_cosine_similarity_test, gold_matrix_train, df_gold_train, gold_matrix_test, df_gold_test)\n",
    "pretty_metrics = pd.DataFrame(pd.Series(metrics), columns = [\"Score\"])\n",
    "pretty_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF+NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Quantile (Rank) Difference score: 0.852964499135 - 0.436339182481 = 0.416625316654\n",
      "Pairwise Binary Logistic Regression Accuracy score: 0.860729200548\n",
      "\n",
      "The next test uses parameter optimization over a random forest\n",
      "classifier's parameters and may take 30s to 2 min to run.\n",
      "\n",
      "Pairwise Binary Random Forest Accuracy score: 0.858749148269\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>logreg_acc_pairwise_binary</th>\n",
       "      <td>0.860729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median_quantile_diff</th>\n",
       "      <td>0.416625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random_forest_acc_pairwise_binary</th>\n",
       "      <td>0.858749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Score\n",
       "logreg_acc_pairwise_binary         0.860729\n",
       "median_quantile_diff               0.416625\n",
       "random_forest_acc_pairwise_binary  0.858749"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def TFIDF_NMF(eigenvector,df):\n",
    "    nmf = NMF(n_components=eigenvector)\n",
    "    A = nmf.fit_transform(df)\n",
    "    A=normalize(A,norm=\"l2\")\n",
    "    simdf=pd.DataFrame(np.dot(A,A.transpose()))\n",
    "    return MinMaxScaler().fit_transform(simdf)\n",
    "\n",
    "eigenvector=20\n",
    "pairwise_cosine_similarity_test=TFIDF_NMF(eigenvector,testdf_tfs)\n",
    "nmf_sim=pd.DataFrame(pairwise_cosine_similarity_test)\n",
    "nmf_sim.to_csv(\"nmf_sim_test.csv\")\n",
    "pairwise_cosine_similarity_train=TFIDF_NMF(eigenvector,traindf_tfs)\n",
    "nmf_sim=pd.DataFrame(pairwise_cosine_similarity_train)\n",
    "nmf_sim.to_csv(\"nmf_sim_train.csv\")\n",
    "\n",
    "metrics = compute_metrics(pairwise_cosine_similarity_train, pairwise_cosine_similarity_test, gold_matrix_train, df_gold_train, gold_matrix_test, df_gold_test)\n",
    "pretty_metrics = pd.DataFrame(pd.Series(metrics), columns = [\"Score\"])\n",
    "pretty_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(649, 3195)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdf_tfs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=20).fit(testdf_tfs.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=eigenvector)\n",
    "df=pd.DataFrame(testdf_tfs.toarray())\n",
    "A=pca.fit_transform(df)\n",
    "A=normalize(A,norm=\"l2\")\n",
    "simdf=pd.DataFrame(np.dot(A,A.transpose()))\n",
    "x= MinMaxScaler().fit_transform(simdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Quantile (Rank) Difference score: 0.866431941045 - 0.428079467997 = 0.438352473047\n",
      "Pairwise Binary Logistic Regression Accuracy score: 0.868571062272\n",
      "\n",
      "The next test uses parameter optimization over a random forest\n",
      "classifier's parameters and may take 30s to 2 min to run.\n",
      "\n",
      "Pairwise Binary Random Forest Accuracy score: 0.86815795784\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>logreg_acc_pairwise_binary</th>\n",
       "      <td>0.868571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median_quantile_diff</th>\n",
       "      <td>0.438352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random_forest_acc_pairwise_binary</th>\n",
       "      <td>0.868158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Score\n",
       "logreg_acc_pairwise_binary         0.868571\n",
       "median_quantile_diff               0.438352\n",
       "random_forest_acc_pairwise_binary  0.868158"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def TFIDF_PCA(eigenvector,df):\n",
    "    pca = PCA(n_components=eigenvector)\n",
    "    df=pd.DataFrame(df.toarray())\n",
    "    A=pca.fit_transform(df)\n",
    "    A=normalize(A,norm=\"l2\")\n",
    "    simdf=pd.DataFrame(np.dot(A,A.transpose()))\n",
    "    return MinMaxScaler().fit_transform(simdf)\n",
    "\n",
    "eigenvector=20\n",
    "pairwise_cosine_similarity_test=TFIDF_PCA(eigenvector,testdf_tfs)\n",
    "nmf_sim=pd.DataFrame(pairwise_cosine_similarity_test)\n",
    "nmf_sim.to_csv(\"pca_sim_test.csv\")\n",
    "pairwise_cosine_similarity_train=TFIDF_PCA(eigenvector,traindf_tfs)\n",
    "nmf_sim=pd.DataFrame(pairwise_cosine_similarity_train)\n",
    "nmf_sim.to_csv(\"pca_sim_train.csv\")\n",
    "\n",
    "metrics = compute_metrics(pairwise_cosine_similarity_train, pairwise_cosine_similarity_test, gold_matrix_train, df_gold_train, gold_matrix_test, df_gold_test)\n",
    "pretty_metrics = pd.DataFrame(pd.Series(metrics), columns = [\"Score\"])\n",
    "pretty_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
