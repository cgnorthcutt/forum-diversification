{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import unicode_literals\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import rankdata\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pairwise_cosine_similarity_train, pairwise_cosine_similarity_test, gold_matrix_train, gold_data_train, gold_matrix_test, gold_data_test):\n",
    "  \n",
    "  # Scale / Normalize pairwise_cosine_similarity\n",
    "  pairwise_cosine_similarity_test = MinMaxScaler().fit_transform(pairwise_cosine_similarity_test)\n",
    "  \n",
    "  # Convert gold matrices to np.array (instead of pd.DataFrame) if necessary\n",
    "  gold_matrix_train = gold_matrix_train.values if isinstance(gold_matrix_train, pd.DataFrame) else gold_matrix_train\n",
    "  gold_matrix_test = gold_matrix_test.values if isinstance(gold_matrix_test, pd.DataFrame) else gold_matrix_test\n",
    "  \n",
    "  # Compute list of indices of sorted matrix values in list form\n",
    "  flat_cossim_train = pairwise_cosine_similarity_train.flatten()\n",
    "  flat_cossim_test = pairwise_cosine_similarity_test.flatten()\n",
    "  flat_gold_train = gold_matrix_train.flatten()\n",
    "  flat_gold_test = gold_matrix_test.flatten()\n",
    "  \n",
    "  npairs_train = float(len(flat_cossim_train))\n",
    "  npairs_test = float(len(flat_cossim_test))\n",
    "  \n",
    "  metrics = {}\n",
    "  \n",
    "  #\n",
    "  # Metric 1: avg_diff (Average difference of cossine similarity for gold score classes)\n",
    "  #\n",
    "  \n",
    "  # Compute avg consine similarity for same cluster comments and different cluster topics and subtract.\n",
    "  same_cluster_avg_score = np.multiply(pairwise_cosine_similarity_test, gold_matrix_test).sum() / gold_matrix_test.sum()\n",
    "  diff_cluster_avg_score = np.multiply(pairwise_cosine_similarity_test, 1-gold_matrix_test).sum() / (1-gold_matrix_test).sum()\n",
    "  metrics[\"avg_diff\"] = same_cluster_avg_score - diff_cluster_avg_score\n",
    "  print(\"Avg Difference score:\", same_cluster_avg_score, \"-\", diff_cluster_avg_score, \"=\", metrics[\"avg_diff\"])\n",
    "  \n",
    "  #\n",
    "  # Metric 1: median_diff (Median difference of cossine similarity for gold score classes)\n",
    "  #\n",
    "  \n",
    "  gold0_values = flat_cossim_test[flat_gold_test == 0]\n",
    "  gold1_values = flat_cossim_test[flat_gold_test == 1]\n",
    "  np.median(gold1_values) - np.median(gold0_values)\n",
    "  metrics[\"median_diff\"] = np.median(gold1_values) - np.median(gold0_values)\n",
    "  print(\"Median Difference score:\", np.median(gold1_values), \"-\", np.median(gold0_values), \"=\", metrics[\"median_diff\"])\n",
    "  \n",
    "  #\n",
    "  # Metric 2: median_quantile_diff (Median normalized rank difference of cossine similarity for gold score classes)\n",
    "  #\n",
    "  \n",
    "  ranks = rankdata(flat_cossim_test)\n",
    "  gold0_values_quantile = ranks[flat_gold_test == 0] / npairs_test\n",
    "  gold1_values_quantile = ranks[flat_gold_test == 1] / npairs_test\n",
    "  np.median(gold1_values_quantile) - np.median(gold0_values_quantile)\n",
    "  metrics[\"median_quantile_diff\"] = np.median(gold1_values_quantile) - np.median(gold0_values_quantile)\n",
    "  print(\"Median Quantile (Rank) Difference score:\", np.median(gold1_values_quantile), \"-\", np.median(gold0_values_quantile), \"=\", metrics[\"median_quantile_diff\"])\n",
    "  sys.stdout.flush()\n",
    "  \n",
    "  #\n",
    "  # Metric 3: logreg_acc_pairwise_binary Binary Logistic Regression Accuracy\n",
    "  # \n",
    "  \n",
    "  clf = LogisticRegression()\n",
    "  clf.fit(flat_cossim_train.reshape((int(npairs_train), 1)), flat_gold_train.reshape((int(npairs_train), 1)))\n",
    "  y_pred = clf.predict(flat_cossim_test.reshape((int(npairs_test), 1)))\n",
    "  acc = accuracy_score(y_true = flat_gold_test, y_pred = y_pred)\n",
    "  metrics[\"logreg_acc_pairwise_binary\"] = acc\n",
    "  print(\"Pairwise Binary Logistic Regression Accuracy score:\", acc)\n",
    "  sys.stdout.flush()\n",
    "  \n",
    "#   #\n",
    "#   # Metric 4: logreg_acc_topic Binary Logistic Regression Accuracy\n",
    "#   # \n",
    "  \n",
    "#   clf = LogisticRegression()\n",
    "#   clf.fit(flat_cossim_train, gold_data)\n",
    "#   y_pred = clf.pred(flat_cossim_test)\n",
    "#   acc = accuracy_score(y_true = flat_gold, y_pred = y_pred)\n",
    "#   metrics[\"logreg_acc_binary\"] = acc\n",
    "#   print(\"Binary Logistic Regression Accuracy score:\", acc)\n",
    "#   sys.stdout.flush()\n",
    "  \n",
    "  return metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
